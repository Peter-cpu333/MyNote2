# 一 模型训练与评测记录

暂时无法在飞书文档外展示此内容

| 基础模型名称    | 参数/量化 | 最大输入/输出token | 微调数据集 | 微调参数设置                       | 代码路径                             | Token输出速度 | 评测维度 | 评测效果打分 | 其他说明 |
| --------------- | --------- | ------------------ | ---------- | ---------------------------------- | ------------------------------------ | ------------- | -------- | ------------ | -------- |
| Qwen3-14B，BF16 | 14B       | 原生               | 中碳语料1  | 学习率：5e-5截断长度：1024epoch：3 | /home/usr1/fine-model/Qwen3-14B-5e5/ | 32tokens      |          |              |          |
| Qwen3-32B，BF16 | 32B/BF16  | 原生               | 中碳语料1  |                                    |                                      |               |          |              |          |
|                 |           |                    |            |                                    |                                      |               |          |              |          |
|                 |           |                    |            |                                    |                                      |               |          |              |          |
| Qwen3-32B，INT8 |           |                    |            |                                    |                                      |               |          |              |          |
| Qwen3-7B，FP16  |           |                    |            |                                    |                                      |               |          |              |          |
| ...             |           |                    |            |                                    |                                      |               |          |              |          |
|                 |           |                    |            |                                    |                                      |               |          |              |          |

# 二 模型训练方法

1.  ## 启动llamafactor

```Bash
# 环境名称是 llama
# 1
conda activate llama
# 2 
llamafactory-cli webui --share=true

# 查看服务器日志
sudo tail -f /var/log/auth.log
```

1.  ## 配置训练参数

1.  模型路径，可以利用modelscope拉去后，选择文件夹路径。/home/usr1/yongqiang/model/Qwen3-32B/
2.  数据集路径：/home/usr1/dataset
3.  显存不够，调整**截断长度**、**批处理大小**

1.  ## 参数量过大的模型

模型参数量过大，需要使用**DeepSpeed stage**，有2，3阶段。

![img](https://lxr5a5dzfbq.feishu.cn/space/api/box/stream/download/asynccode/?code=NWY1YmQ0NDRlYzE0MGViYTdmMDk2OTMyNzdhNzA1MmRfeVRWQ2FHN2txM0gwU2YxS1loSGF5cWloMFVoc1E1N0pfVG9rZW46Q2pueGJBbXhrb0VPZE54d0RvTGNqaWVtblFnXzE3NTg1NDk3NTg6MTc1ODU1MzM1OF9WNA)

1.  ## 训练设置完成可以保存 训练命令

保存训练命令用于管理

```Bash
llamafactory-cli train \    
    --stage sft \     
    --do_train True \     
    --model_name_or_path /home/usr1/yongqiang/model/Qwen3-32B/ \     
    --preprocessing_num_workers 16 \     
    --finetuning_type lora \     
    --template qwen3 \     
    --flash_attn auto \     
    --dataset_dir /home/usr1/dataset \     
    --dataset train_data \     
    --cutoff_len 512 \     
    --learning_rate 2e-05 \     
    --num_train_epochs 3.0 \     
    --max_samples 100000 \     
    --per_device_train_batch_size 4 \     
    --gradient_accumulation_steps 8 \     
    --lr_scheduler_type cosine \     
    --max_grad_norm 1.0 \     
    --logging_steps 5 \     
    --save_steps 100 \     
    --warmup_steps 0 \     
    --packing False \     
    --enable_thinking True \     
    --report_to none \     
    --output_dir saves/Qwen3-32B-Instruct/lora/train_2025-06-18-16-21-43 \     
    --bf16 True \     
    --plot_loss True \     
    --trust_remote_code True \     
    --ddp_timeout 180000000 \     
    --include_num_input_tokens_seen True \     
    --optim adamw_torch \     
    --lora_rank 8 \     
    --lora_alpha 16 \     
    --lora_dropout 0 \     
    --lora_target all \     
    --deepspeed cache/ds_z3_config.json
```

# 三 Dify私有部署环境

1.  ## docker启动Dify；vLLM环境启动大模型接口。

1.  ## 配置 vLLM 接口

首先在Dify模型提供商中选择vLLM。部分参数选择：

1.  **模型名称：**/home/usr1/fine-model/Qwen3-14B-5e5/      模型的位置文件夹
2.  **API endpoint URL：** http://192.168.0.109:8000/v1   
3.  其他参数可以进行调配。

![img](https://lxr5a5dzfbq.feishu.cn/space/api/box/stream/download/asynccode/?code=ZmY2YTQwMGZlYjAzYmY5ZjFmOTk5ZTU4YjVkODc5NmZfUkxPZzg3Sm9tZXA0MUl3TjhMc3kwQ1NRMURFRFlwMXVfVG9rZW46WWJyWmJRaVRQb2p4eGR4cjQ2bGNhUkJwbmFkXzE3NTg1NDk3NTg6MTc1ODU1MzM1OF9WNA)

1.  ## 配置 llama.cpp 接口

也可以选择模型提供商vLLM。

1.  **模型名称：**模型的位置文件夹
2.  **API endpoint URL：** [http://192.168.0.109:6006/v1](http://192.168.0.109:8000/v1)   
3.  其他参数可以进行调配。

# 四 Llama.cpp启动

需要在llama.cpp文件下启动：cd llama.cpp

## 一、操作步骤

### （一）模型转换：将微调后模型转为 GGUF 文件

微调后的模型需转换为 GGUF 格式，才可在 llama.cpp 中运行。以 Qwen3 - 8B 为例（实际使用时，`Qwen/Qwen3-8B` 替换为目标模型路径 ），执行如下命令：

```Bash
cd llama.cpp
python convert_hf_to_gguf.py /home/usr1/fine-model/Qwen3-32B-0723 --outfile qwen3-32b-0723-f16.gguf
```

### （二）启动方式

#### 1. 对话格式启动

通过 `llama-cli` 以对话交互形式启动模型，适合直接测试模型对话能力，命令如下：

```Bash
./build/bin/llama-cli -m /home/usr1/fine-model/Qwen3-32B-V3/Qwen3-32B-V3-F16.gguf \
  -cnv \
  --n-gpu-layers 2000
```

参数说明：

-   `-m`：指定 GGUF 格式模型文件路径，需替换为实际模型存储路径。
-   `-cnv`：启用对话模式相关配置（具体行为依 llama.cpp 实现，用于适配对话交互逻辑 ）。 
-   `--n-gpu-layers 2000`：设置加载到 GPU 的层数，2000 为示例值，可依据 GPU 显存、模型规模调整，充分利用 GPU 加速推理。

#### 2. OpenAI API 格式启动

通过 `llama-server` 启动，模拟 OpenAI API 服务，方便与依赖 OpenAI 接口格式的应用集成，命令如下：

```Bash
./build/bin/llama-server -m /home/usr1/fine-model/Qwen3-32B-V3/Qwen3-32B-V3-F16.gguf \
  --port 6006 \
  --n-gpu-layers 2000 \
  -ts 1,1 
```

参数说明：

-   `-m`：同对话模式，指定模型文件路径。 
-   `--port 6006`：设置服务监听端口，启动后可通过 `http://<服务器IP>:6006` 访问 API 服务。 
-   `--n-gpu-layers 2000`：GPU 层加载配置，作用同对话模式。 
-   `-ts 1,1`：将计算负载平均分配到两张 GPU 卡，适配多卡环境，提升推理效率。 

# 五 vLLM启动

需要在vllm环境中启动：conda activate vllm-env

## 一、服务启动

### （一）单卡部署

```Bash
CUDA_VISIBLE_DEVICES=3 python3 -m vllm.entrypoints.openai.api_server \
  --model /home/usr1/yongqiang/model/Qwen3-14B/ \
  --host 0.0.0.0 \
  --port 8001 \
  --gpu-memory-utilization 0.7
```

### （二）多卡部署（以使用 GPU 0,1,2,3 为例）

1.  指定特定 GPU 并设置张量并行

```Bash
CUDA_VISIBLE_DEVICES=2,3 python3 -m vllm.entrypoints.openai.api_server \
  --model /home/usr1/fine-model/Qwen3-32B-0723/ \
  --host 0.0.0.0 \
  --port 8003 \
  --tensor-parallel-size 2 \
  --gpu-memory-utilization 0.7
```

此配置使用 GPU 2 和 3（`CUDA_VISIBLE_DEVICES=2,3`），并将模型按张量并行方式拆分为 2 份（`--tensor-parallel-size 2`），均匀分布在两张卡上运行。

1.  多卡全量部署（使用 4 张卡）

若需使用全部 4 张 GPU（0,1,2,3），可直接设置张量并行大小为 4：

```Bash
CUDA_VISIBLE_DEVICES=0,1,2,3 python3 -m vllm.entrypoints.openai.api_server \
  --model /home/usr1/fine-model/Qwen3-32B-V3/ \
  --host 0.0.0.0 \
  --port 8002 \
  --tensor-parallel-size 4
```

## 二、 修改模型配置

但是，如果需要修改模型的max_tokens，需要在模型文件中的config.json中的参数。原始rope_scaling的值是'**null**'。里面主要参数是**factor**，表示的是相比原始长度的倍数，根据显存进行调整。**32768**是原始最大长度。

```Bash
"rope_scaling": {
    "rope_type": "yarn",
    "factor": 2.5,
    "original_max_position_embeddings": 32768
}
```

# 六 手动评测计划

有一个新的文档：[模型手动测试](https://lxr5a5dzfbq.feishu.cn/wiki/Ft40wm7BOi0segkWhCHc6KhUn5f)

**待补充**

| 问题 | 原始答案 | Qwen3-14B | Qwen3-14B-V2 | Qwen3-14B-V3 | Qwen3-32B-V3 |      |
| ---- | -------- | --------- | ------------ | ------------ | ------------ | ---- |
|      |          |           |              |              |              |      |

# 七 语料生成prompt

```Bash
任务背景：
你是一位专注于碳中和领域的专家。你的目标是基于标准文档，生成高质量的问答对，用于构建标准知识库或训练大模型回答法规政策类问题。
任务要求：
根据文章的主要章节结构（如一级标题和二级标题），生成概括性问题
问题需覆盖原文的主要章节内容，且具有较强的针对性和概括性。
示例问题：
钢铁行业节能降碳专项行动计划的主要目标是什么？
钢铁行业节能降碳专项行动计划的重点任务有哪些，并详细说明？
从文章中提取完整的章节内容作为回答
引用的内容需保持原文完整性，不删减、不截断。
输出格式要求如下：
Q： （问题）
A： （回答）
避免使用“本文件”“本指南”“该文件”“该指南”等模糊指代表述
若原文中包含类似表述，应根据上下文替换为具体的文件名称或主题描述。
请在回答中明确标注所引用内容的原始出处。
示例：
原文表述：“本指南在编制过程中参考了多项资料和实践经验。”
替换为：“碳中和行动指南在编制过程中参考了哪些资料和实践经验？”
问题需以具体内容为导向，避免出现模糊或泛指的表述
```

暂时无法在飞书文档外展示此内容

# 八 评测计划

将分为三个维度进行

1.  ## 领域知识评估

### 评估指标

领域知识的评估主要采用以下三个指标：

-   **BLEU Score**：只考虑词汇匹配，不考虑语义
-   **ROUGE Score**：基于召回率计算，关注参考文本中有多少内容被生成文本覆盖
-   **Semantic Similarity Score**：评估两个文本在语义层面的相似程度

### 评估集构建

领域知识的评估集基于**中碳语料1**和**碳中和语料250626**进行构建，具体方案如下：

### 数据抽取策略

-   **训练集抽取**：各语料抽取200条，用于了解模型学习程度
-   **测试集抽取**：各语料抽取100条，用于了解模型发散程度
-   对碳中和语料250626的原始切片，随机构建100条新的综合性、全面性问题，以此判断模型是否能够很好地整合知识。

目前已完成评价脚本，待评价。

1.  ## 通用知识

1.  ## 安全性、 敏感性

1.  ### **语料采集前后违法信息内容均不超过5%，语料来源可追溯**

1.  ### **知识产权问题**

1.  ### 关键词库

1.  ### 用户输入监测

使用者连续三次或 一天内累计五次输入违法不良信息或明显诱导生成违法不良信息的，应依法依约采取暂停提供服务等处置措施

对明显偏激以及明显诱导生成违法不良信息的问题，应拒绝回答；对其他问题， 应均能正常回答；

1.  ## opencompass评测

评测结果：[微调任务管理](https://lxr5a5dzfbq.feishu.cn/wiki/HBBmwYQYyiK8Gok4yHxcLXqBnbY?table=tbl3NRc02HolH5UF&view=vewQMmJeOX)

-   查看可用模型和数据集：

Python tools/list_configs.py qwen3

-   运行指令：

环境：opencompass

目标：/home/usr1/opencompass

CUDA_VISIBLE_DEVICES=2 python run.py --models Qwen3-14B-0703 --datasets ceval_clean_ppl --debug